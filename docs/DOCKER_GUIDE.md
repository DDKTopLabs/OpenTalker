# OpenTalker Docker éƒ¨ç½²æŒ‡å—

## ğŸ“‹ å‰ç½®è¦æ±‚

### ç¡¬ä»¶è¦æ±‚

#### æ¨èé…ç½®ï¼ˆå®Œæ•´åŠŸèƒ½ï¼‰
- **GPU**: NVIDIA GPU with CUDA support (æ¨è RTX 2080 Ti æˆ–æ›´é«˜)
- **æ˜¾å­˜**: è‡³å°‘ 8GB (æ¨è 22GB+)
- **å†…å­˜**: è‡³å°‘ 16GB
- **ç£ç›˜**: è‡³å°‘ 20GB å¯ç”¨ç©ºé—´

#### æœ€ä½é…ç½®ï¼ˆå•æœåŠ¡æ¨¡å¼ï¼‰
- **GPU**: NVIDIA GTX 1050 Ti æˆ–æ›´é«˜
- **æ˜¾å­˜**: 4GB (ä»…æ”¯æŒå•ä¸ªæœåŠ¡ï¼šSTT æˆ– TTS)
- **å†…å­˜**: è‡³å°‘ 8GB
- **ç£ç›˜**: è‡³å°‘ 15GB å¯ç”¨ç©ºé—´

> âš ï¸ **4GB æ˜¾å­˜é™åˆ¶è¯´æ˜**ï¼š
> - STT æœåŠ¡å ç”¨çº¦ **3.1GB æ˜¾å­˜**
> - TTS æœåŠ¡å ç”¨çº¦ **2.2GB æ˜¾å­˜**
> - 4GB æ˜¾å­˜çš„ GPU **æ— æ³•åŒæ—¶è¿è¡Œ** STT å’Œ TTS æœåŠ¡
> - å»ºè®®ä½¿ç”¨**å•æœåŠ¡éƒ¨ç½²æ¨¡å¼**ï¼ˆè§ä¸‹æ–‡"4GB æ˜¾å­˜éƒ¨ç½²æ–¹æ¡ˆ"ï¼‰

### è½¯ä»¶è¦æ±‚
- **Docker**: 20.10+
- **Docker Compose**: 2.0+
- **NVIDIA Container Toolkit**: ç”¨äºGPUæ”¯æŒ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…NVIDIA Container Toolkit

**Ubuntu/Debian:**
```bash
# æ·»åŠ ä»“åº“
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

# å®‰è£…
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# é‡å¯Docker
sudo systemctl restart docker
```

**éªŒè¯å®‰è£…:**
```bash
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

### 2. ç¼–è¯‘Dockeré•œåƒ

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/DDKTopLabs/OpenTalker.git
cd OpenTalker

# ç¼–è¯‘æ‰€æœ‰é•œåƒ
./build_docker.sh
```

ç¼–è¯‘æ—¶é—´çº¦15-30åˆ†é’Ÿï¼Œå–å†³äºç½‘ç»œé€Ÿåº¦ã€‚

### 3. å¯åŠ¨æœåŠ¡

```bash
# ä½¿ç”¨docker-composeå¯åŠ¨
docker-compose -f docker-compose.workspace.yml up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose -f docker-compose.workspace.yml logs -f

# ç­‰å¾…æœåŠ¡å¯åŠ¨ï¼ˆçº¦1-2åˆ†é’Ÿï¼‰
```

### 4. éªŒè¯æœåŠ¡

```bash
# æ£€æŸ¥æ‰€æœ‰æœåŠ¡çŠ¶æ€
curl http://localhost:8000/health | jq

# æ£€æŸ¥STTæœåŠ¡
curl http://localhost:8001/health | jq

# æ£€æŸ¥TTSæœåŠ¡
curl http://localhost:8002/health | jq
```

## ğŸ¯ 4GB æ˜¾å­˜éƒ¨ç½²æ–¹æ¡ˆ

å¦‚æœä½ çš„ GPU åªæœ‰ 4GB æ˜¾å­˜ï¼ˆå¦‚ GTX 1050 Tiï¼‰ï¼Œæ— æ³•åŒæ—¶è¿è¡Œ STT å’Œ TTS æœåŠ¡ã€‚ä»¥ä¸‹æ˜¯æ¨èçš„éƒ¨ç½²æ–¹æ¡ˆï¼š

### æ–¹æ¡ˆä¸€ï¼šä»…éƒ¨ç½² STT æœåŠ¡ï¼ˆè¯­éŸ³è¯†åˆ«ï¼‰

åˆ›å»º `docker-compose-stt-only.yml`ï¼š

```yaml
version: '3.8'

services:
  stt:
    image: ghcr.io/ddktoplabs/opentalker-stt:v0.3.0-optimized
    container_name: opentalker-stt
    ports:
      - "8001:8001"
    environment:
      - MODEL_NAME=Qwen/Qwen3-ASR-0.6B
      - DEVICE=cuda
      - LOG_LEVEL=INFO
      - HF_ENDPOINT=https://hf-mirror.com
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
```

å¯åŠ¨æœåŠ¡ï¼š
```bash
docker compose -f docker-compose-stt-only.yml up -d
```

### æ–¹æ¡ˆäºŒï¼šä»…éƒ¨ç½² TTS æœåŠ¡ï¼ˆè¯­éŸ³åˆæˆï¼‰

åˆ›å»º `docker-compose-tts-only.yml`ï¼š

```yaml
version: '3.8'

services:
  tts:
    image: ghcr.io/ddktoplabs/opentalker-tts:v0.3.0-optimized
    container_name: opentalker-tts
    ports:
      - "8002:8002"
    environment:
      - MODEL_NAME=Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice
      - DEVICE=cuda
      - LOG_LEVEL=INFO
      - HF_ENDPOINT=https://hf-mirror.com
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
```

å¯åŠ¨æœåŠ¡ï¼š
```bash
docker compose -f docker-compose-tts-only.yml up -d
```

### æ–¹æ¡ˆä¸‰ï¼šä½¿ç”¨ä¼˜åŒ–é•œåƒï¼ˆæ¨èï¼‰

ä¼˜åŒ–é•œåƒç›¸æ¯”æ ‡å‡†é•œåƒå‡å°‘äº† **38% çš„å¤§å°**ï¼š

| é•œåƒç±»å‹ | æ ‡å‡†ç‰ˆ | ä¼˜åŒ–ç‰ˆ | èŠ‚çœ |
|---------|--------|--------|------|
| STT | 5.55GB | 3.44GB | 2.11GB |
| TTS | 5.49GB | 3.39GB | 2.10GB |

ä½¿ç”¨ä¼˜åŒ–é•œåƒåªéœ€å°†ä¸Šè¿°é…ç½®ä¸­çš„é•œåƒæ ‡ç­¾æ”¹ä¸ºï¼š
- `ghcr.io/ddktoplabs/opentalker-stt:v0.3.0-optimized`
- `ghcr.io/ddktoplabs/opentalker-tts:v0.3.0-optimized`

### æ˜¾å­˜ä½¿ç”¨æƒ…å†µ

| æœåŠ¡ | æ˜¾å­˜å ç”¨ | 4GB GPU å…¼å®¹æ€§ |
|------|---------|---------------|
| STT å•ç‹¬è¿è¡Œ | ~3.1GB | âœ… å¯ç”¨ |
| TTS å•ç‹¬è¿è¡Œ | ~2.2GB | âœ… å¯ç”¨ |
| STT + TTS åŒæ—¶è¿è¡Œ | ~4.0GB+ | âŒ è¶…å‡ºå®¹é‡ |

### ä¸­å›½ç”¨æˆ·åŠ é€Ÿ

ä½¿ç”¨ä¸­å›½é•œåƒæºåŠ é€Ÿä¸‹è½½ï¼š

```yaml
environment:
  - HF_ENDPOINT=https://hf-mirror.com  # HuggingFace æ¨¡å‹é•œåƒ
```

é•œåƒæ‹‰å–ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸­å›½é•œåƒï¼š
```bash
# ä½¿ç”¨ ghcr.1ms.run é•œåƒï¼ˆä¸­å›½åŠ é€Ÿï¼‰
docker pull ghcr.1ms.run/ddktoplabs/opentalker-stt:v0.3.0-optimized
docker tag ghcr.1ms.run/ddktoplabs/opentalker-stt:v0.3.0-optimized \
  ghcr.io/ddktoplabs/opentalker-stt:v0.3.0-optimized
```

## ğŸ“¦ é•œåƒè¯´æ˜

### Gateway é•œåƒ
- **åŸºç¡€é•œåƒ**: python:3.11-slim
- **å¤§å°**: ~200MB (å‹ç¼©å 82.5MB)
- **ç”¨é€”**: API ç½‘å…³ï¼Œè·¯ç”±è¯·æ±‚
- **GPU**: ä¸éœ€è¦

### STT Service é•œåƒ

#### æ ‡å‡†ç‰ˆ
- **é•œåƒ**: `ghcr.io/ddktoplabs/opentalker-stt:v0.3.0`
- **åŸºç¡€é•œåƒ**: nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04
- **å¤§å°**: ~15.8GB (å‹ç¼©å 5.55GB)
- **ç”¨é€”**: è¯­éŸ³è¯†åˆ« (Qwen3-ASR-0.6B)
- **GPU**: éœ€è¦ (~3.1GB æ˜¾å­˜)

#### ä¼˜åŒ–ç‰ˆï¼ˆæ¨èï¼‰
- **é•œåƒ**: `ghcr.io/ddktoplabs/opentalker-stt:v0.3.0-optimized`
- **åŸºç¡€é•œåƒ**: nvidia/cuda:12.1.0-base-ubuntu22.04
- **å¤§å°**: ~10.3GB (å‹ç¼©å 3.44GB)
- **ç”¨é€”**: è¯­éŸ³è¯†åˆ« (Qwen3-ASR-0.6B)
- **GPU**: éœ€è¦ (~3.1GB æ˜¾å­˜)
- **ä¼˜åŠ¿**: é•œåƒä½“ç§¯å‡å°‘ 38%ï¼ŒåŠŸèƒ½å®Œå…¨ç›¸åŒ

### TTS Service é•œåƒ

#### æ ‡å‡†ç‰ˆ
- **é•œåƒ**: `ghcr.io/ddktoplabs/opentalker-tts:v0.3.0`
- **åŸºç¡€é•œåƒ**: nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04
- **å¤§å°**: ~15.6GB (å‹ç¼©å 5.49GB)
- **ç”¨é€”**: è¯­éŸ³åˆæˆ (Qwen3-TTS-12Hz-0.6B)
- **GPU**: éœ€è¦ (~2.2GB æ˜¾å­˜)

#### ä¼˜åŒ–ç‰ˆï¼ˆæ¨èï¼‰
- **é•œåƒ**: `ghcr.io/ddktoplabs/opentalker-tts:v0.3.0-optimized`
- **åŸºç¡€é•œåƒ**: nvidia/cuda:12.1.0-base-ubuntu22.04
- **å¤§å°**: ~10.1GB (å‹ç¼©å 3.39GB)
- **ç”¨é€”**: è¯­éŸ³åˆæˆ (Qwen3-TTS-12Hz-0.6B)
- **GPU**: éœ€è¦ (~2.2GB æ˜¾å­˜)
- **ä¼˜åŠ¿**: é•œåƒä½“ç§¯å‡å°‘ 38%ï¼ŒåŠŸèƒ½å®Œå…¨ç›¸åŒ
- **GPU**: éœ€è¦ (~3GBæ˜¾å­˜)

## ğŸ”§ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡

**Gateway:**
```yaml
GATEWAY_HOST: 0.0.0.0
GATEWAY_PORT: 8000
STT_SERVICE_URL: http://stt-service:8001
TTS_SERVICE_URL: http://tts-service:8002
STT_TIMEOUT: 120
TTS_TIMEOUT: 180
LOG_LEVEL: INFO
```

**STT Service:**
```yaml
SERVICE_HOST: 0.0.0.0
SERVICE_PORT: 8001
QWEN_ASR_MODEL: Qwen/Qwen3-ASR-0.6B
QWEN_ASR_DEVICE: cuda:0
QWEN_ASR_MAX_BATCH_SIZE: 8
MAX_UPLOAD_SIZE: 52428800
HF_ENDPOINT: https://hf-mirror.com
LOG_LEVEL: INFO
```

**TTS Service:**
```yaml
SERVICE_HOST: 0.0.0.0
SERVICE_PORT: 8002
QWEN_TTS_MODEL: Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice
QWEN_TTS_DEVICE: cuda:0
HF_ENDPOINT: https://hf-mirror.com
LOG_LEVEL: INFO
```

### ç«¯å£æ˜ å°„

- **8000**: Gateway (OpenAIå…¼å®¹API)
- **8001**: STT Service (è¯­éŸ³è¯†åˆ«)
- **8002**: TTS Service (è¯­éŸ³åˆæˆ)

### æ•°æ®å·

```yaml
volumes:
  - ./models:/models              # æ¨¡å‹ç¼“å­˜ç›®å½•
  - ./stt-service/tmp:/app/tmp   # STTä¸´æ—¶æ–‡ä»¶
```

## ğŸ§ª æµ‹è¯•API

### STT (è¯­éŸ³è¯†åˆ«)

```bash
# é€šè¿‡Gateway
curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F 'file=@audio.wav' \
  -F 'model=qwen3-asr' \
  -F 'language=Chinese'

# ç›´æ¥è°ƒç”¨STTæœåŠ¡
curl -X POST http://localhost:8001/transcribe \
  -F 'file=@audio.wav' \
  -F 'language=Chinese'
```

### TTS (è¯­éŸ³åˆæˆ)

```bash
# ç›´æ¥è°ƒç”¨TTSæœåŠ¡
curl -X POST http://localhost:8002/synthesize \
  -H 'Content-Type: application/json' \
  -d '{
    "input": "ä½ å¥½ä¸–ç•Œ",
    "speaker": "vivian",
    "language": "Chinese"
  }' \
  -o output.wav
```

## ğŸ“Š ç›‘æ§å’Œç®¡ç†

### æŸ¥çœ‹å®¹å™¨çŠ¶æ€
```bash
docker-compose -f docker-compose.workspace.yml ps
```

### æŸ¥çœ‹èµ„æºä½¿ç”¨
```bash
docker stats
```

### æŸ¥çœ‹GPUä½¿ç”¨
```bash
docker exec opentalker-stt nvidia-smi
```

### æŸ¥çœ‹æ—¥å¿—
```bash
# æ‰€æœ‰æœåŠ¡
docker-compose -f docker-compose.workspace.yml logs -f

# ç‰¹å®šæœåŠ¡
docker-compose -f docker-compose.workspace.yml logs -f gateway
docker-compose -f docker-compose.workspace.yml logs -f stt-service
docker-compose -f docker-compose.workspace.yml logs -f tts-service
```

### é‡å¯æœåŠ¡
```bash
# é‡å¯æ‰€æœ‰æœåŠ¡
docker-compose -f docker-compose.workspace.yml restart

# é‡å¯ç‰¹å®šæœåŠ¡
docker-compose -f docker-compose.workspace.yml restart stt-service
```

### åœæ­¢æœåŠ¡
```bash
docker-compose -f docker-compose.workspace.yml down
```

### æ¸…ç†
```bash
# åœæ­¢å¹¶åˆ é™¤å®¹å™¨
docker-compose -f docker-compose.workspace.yml down

# åˆ é™¤é•œåƒ
docker rmi opentalker/gateway:latest
docker rmi opentalker/stt-service:latest
docker rmi opentalker/tts-service:latest

# æ¸…ç†æœªä½¿ç”¨çš„é•œåƒå’Œå®¹å™¨
docker system prune -a
```

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜1: GPUä¸å¯ç”¨

**ç—‡çŠ¶**: æœåŠ¡å¯åŠ¨ä½†æ— æ³•ä½¿ç”¨GPU

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥NVIDIAé©±åŠ¨
nvidia-smi

# æ£€æŸ¥Docker GPUæ”¯æŒ
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# é‡å¯Docker
sudo systemctl restart docker
```

### é—®é¢˜2: æ¨¡å‹ä¸‹è½½å¤±è´¥

**ç—‡çŠ¶**: æœåŠ¡å¯åŠ¨æ—¶å¡åœ¨æ¨¡å‹ä¸‹è½½

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨å›½å†…é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–é¢„å…ˆä¸‹è½½æ¨¡å‹åˆ°./modelsç›®å½•
```

### é—®é¢˜3: å†…å­˜ä¸è¶³

**ç—‡çŠ¶**: å®¹å™¨OOM (Out of Memory)

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# åœ¨docker-compose.ymlä¸­é™åˆ¶å†…å­˜
services:
  stt-service:
    mem_limit: 8g
    memswap_limit: 8g
```

### é—®é¢˜4: ç«¯å£å†²çª

**ç—‡çŠ¶**: ç«¯å£å·²è¢«å ç”¨

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
lsof -i :8000
lsof -i :8001
lsof -i :8002

# ä¿®æ”¹docker-compose.ymlä¸­çš„ç«¯å£æ˜ å°„
ports:
  - "18000:8000"  # ä½¿ç”¨å…¶ä»–ç«¯å£
```

## ğŸ” ç”Ÿäº§éƒ¨ç½²å»ºè®®

### 1. ä½¿ç”¨åå‘ä»£ç†
```nginx
# Nginxé…ç½®ç¤ºä¾‹
upstream opentalker {
    server localhost:8000;
}

server {
    listen 80;
    server_name api.example.com;

    location / {
        proxy_pass http://opentalker;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### 2. æ·»åŠ è®¤è¯
```python
# åœ¨Gatewayä¸­æ·»åŠ API KeyéªŒè¯
from fastapi import Security, HTTPException
from fastapi.security import APIKeyHeader

api_key_header = APIKeyHeader(name="X-API-Key")

async def verify_api_key(api_key: str = Security(api_key_header)):
    if api_key != os.getenv("API_KEY"):
        raise HTTPException(status_code=403, detail="Invalid API Key")
```

### 3. é…ç½®æ—¥å¿—
```yaml
# docker-compose.yml
services:
  gateway:
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
```

### 4. å¥åº·æ£€æŸ¥å’Œè‡ªåŠ¨é‡å¯
```yaml
# docker-compose.yml
services:
  stt-service:
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [Dockerå®˜æ–¹æ–‡æ¡£](https://docs.docker.com/)
- [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-docker)
- [OpenTalker GitHub](https://github.com/DDKTopLabs/OpenTalker)
- [Qwen3-ASR](https://github.com/QwenLM/Qwen3-ASR)
- [Qwen3-TTS](https://github.com/QwenLM/Qwen3-TTS)

---

**ç‰ˆæœ¬**: v0.3.0  
**æ›´æ–°æ—¶é—´**: 2026-02-04  
**ç»´æŠ¤è€…**: DDKTopLabs
