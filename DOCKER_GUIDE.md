# OpenTalker Docker éƒ¨ç½²æŒ‡å—

## ğŸ“‹ å‰ç½®è¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **GPU**: NVIDIA GPU with CUDA support (æ¨èRTX 2080 Tiæˆ–æ›´é«˜)
- **æ˜¾å­˜**: è‡³å°‘8GB (æ¨è22GB+)
- **å†…å­˜**: è‡³å°‘16GB
- **ç£ç›˜**: è‡³å°‘20GBå¯ç”¨ç©ºé—´

### è½¯ä»¶è¦æ±‚
- **Docker**: 20.10+
- **Docker Compose**: 2.0+
- **NVIDIA Container Toolkit**: ç”¨äºGPUæ”¯æŒ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…NVIDIA Container Toolkit

**Ubuntu/Debian:**
```bash
# æ·»åŠ ä»“åº“
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

# å®‰è£…
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# é‡å¯Docker
sudo systemctl restart docker
```

**éªŒè¯å®‰è£…:**
```bash
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

### 2. ç¼–è¯‘Dockeré•œåƒ

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/DDKTopLabs/OpenTalker.git
cd OpenTalker

# ç¼–è¯‘æ‰€æœ‰é•œåƒ
./build_docker.sh
```

ç¼–è¯‘æ—¶é—´çº¦15-30åˆ†é’Ÿï¼Œå–å†³äºç½‘ç»œé€Ÿåº¦ã€‚

### 3. å¯åŠ¨æœåŠ¡

```bash
# ä½¿ç”¨docker-composeå¯åŠ¨
docker-compose -f docker-compose.workspace.yml up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose -f docker-compose.workspace.yml logs -f

# ç­‰å¾…æœåŠ¡å¯åŠ¨ï¼ˆçº¦1-2åˆ†é’Ÿï¼‰
```

### 4. éªŒè¯æœåŠ¡

```bash
# æ£€æŸ¥æ‰€æœ‰æœåŠ¡çŠ¶æ€
curl http://localhost:8000/health | jq

# æ£€æŸ¥STTæœåŠ¡
curl http://localhost:8001/health | jq

# æ£€æŸ¥TTSæœåŠ¡
curl http://localhost:8002/health | jq
```

## ğŸ“¦ é•œåƒè¯´æ˜

### Gatewayé•œåƒ
- **åŸºç¡€é•œåƒ**: python:3.11-slim
- **å¤§å°**: ~200MB
- **ç”¨é€”**: APIç½‘å…³ï¼Œè·¯ç”±è¯·æ±‚
- **GPU**: ä¸éœ€è¦

### STT Serviceé•œåƒ
- **åŸºç¡€é•œåƒ**: nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04
- **å¤§å°**: ~8GB
- **ç”¨é€”**: è¯­éŸ³è¯†åˆ« (Qwen3-ASR-0.6B)
- **GPU**: éœ€è¦ (~3GBæ˜¾å­˜)

### TTS Serviceé•œåƒ
- **åŸºç¡€é•œåƒ**: nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04
- **å¤§å°**: ~8GB
- **ç”¨é€”**: è¯­éŸ³åˆæˆ (Qwen3-TTS-12Hz-0.6B)
- **GPU**: éœ€è¦ (~3GBæ˜¾å­˜)

## ğŸ”§ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡

**Gateway:**
```yaml
GATEWAY_HOST: 0.0.0.0
GATEWAY_PORT: 8000
STT_SERVICE_URL: http://stt-service:8001
TTS_SERVICE_URL: http://tts-service:8002
STT_TIMEOUT: 120
TTS_TIMEOUT: 180
LOG_LEVEL: INFO
```

**STT Service:**
```yaml
SERVICE_HOST: 0.0.0.0
SERVICE_PORT: 8001
QWEN_ASR_MODEL: Qwen/Qwen3-ASR-0.6B
QWEN_ASR_DEVICE: cuda:0
QWEN_ASR_MAX_BATCH_SIZE: 8
MAX_UPLOAD_SIZE: 52428800
HF_ENDPOINT: https://hf-mirror.com
LOG_LEVEL: INFO
```

**TTS Service:**
```yaml
SERVICE_HOST: 0.0.0.0
SERVICE_PORT: 8002
QWEN_TTS_MODEL: Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice
QWEN_TTS_DEVICE: cuda:0
HF_ENDPOINT: https://hf-mirror.com
LOG_LEVEL: INFO
```

### ç«¯å£æ˜ å°„

- **8000**: Gateway (OpenAIå…¼å®¹API)
- **8001**: STT Service (è¯­éŸ³è¯†åˆ«)
- **8002**: TTS Service (è¯­éŸ³åˆæˆ)

### æ•°æ®å·

```yaml
volumes:
  - ./models:/models              # æ¨¡å‹ç¼“å­˜ç›®å½•
  - ./stt-service/tmp:/app/tmp   # STTä¸´æ—¶æ–‡ä»¶
```

## ğŸ§ª æµ‹è¯•API

### STT (è¯­éŸ³è¯†åˆ«)

```bash
# é€šè¿‡Gateway
curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F 'file=@audio.wav' \
  -F 'model=qwen3-asr' \
  -F 'language=Chinese'

# ç›´æ¥è°ƒç”¨STTæœåŠ¡
curl -X POST http://localhost:8001/transcribe \
  -F 'file=@audio.wav' \
  -F 'language=Chinese'
```

### TTS (è¯­éŸ³åˆæˆ)

```bash
# ç›´æ¥è°ƒç”¨TTSæœåŠ¡
curl -X POST http://localhost:8002/synthesize \
  -H 'Content-Type: application/json' \
  -d '{
    "input": "ä½ å¥½ä¸–ç•Œ",
    "speaker": "vivian",
    "language": "Chinese"
  }' \
  -o output.wav
```

## ğŸ“Š ç›‘æ§å’Œç®¡ç†

### æŸ¥çœ‹å®¹å™¨çŠ¶æ€
```bash
docker-compose -f docker-compose.workspace.yml ps
```

### æŸ¥çœ‹èµ„æºä½¿ç”¨
```bash
docker stats
```

### æŸ¥çœ‹GPUä½¿ç”¨
```bash
docker exec opentalker-stt nvidia-smi
```

### æŸ¥çœ‹æ—¥å¿—
```bash
# æ‰€æœ‰æœåŠ¡
docker-compose -f docker-compose.workspace.yml logs -f

# ç‰¹å®šæœåŠ¡
docker-compose -f docker-compose.workspace.yml logs -f gateway
docker-compose -f docker-compose.workspace.yml logs -f stt-service
docker-compose -f docker-compose.workspace.yml logs -f tts-service
```

### é‡å¯æœåŠ¡
```bash
# é‡å¯æ‰€æœ‰æœåŠ¡
docker-compose -f docker-compose.workspace.yml restart

# é‡å¯ç‰¹å®šæœåŠ¡
docker-compose -f docker-compose.workspace.yml restart stt-service
```

### åœæ­¢æœåŠ¡
```bash
docker-compose -f docker-compose.workspace.yml down
```

### æ¸…ç†
```bash
# åœæ­¢å¹¶åˆ é™¤å®¹å™¨
docker-compose -f docker-compose.workspace.yml down

# åˆ é™¤é•œåƒ
docker rmi opentalker/gateway:latest
docker rmi opentalker/stt-service:latest
docker rmi opentalker/tts-service:latest

# æ¸…ç†æœªä½¿ç”¨çš„é•œåƒå’Œå®¹å™¨
docker system prune -a
```

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜1: GPUä¸å¯ç”¨

**ç—‡çŠ¶**: æœåŠ¡å¯åŠ¨ä½†æ— æ³•ä½¿ç”¨GPU

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥NVIDIAé©±åŠ¨
nvidia-smi

# æ£€æŸ¥Docker GPUæ”¯æŒ
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# é‡å¯Docker
sudo systemctl restart docker
```

### é—®é¢˜2: æ¨¡å‹ä¸‹è½½å¤±è´¥

**ç—‡çŠ¶**: æœåŠ¡å¯åŠ¨æ—¶å¡åœ¨æ¨¡å‹ä¸‹è½½

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨å›½å†…é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–é¢„å…ˆä¸‹è½½æ¨¡å‹åˆ°./modelsç›®å½•
```

### é—®é¢˜3: å†…å­˜ä¸è¶³

**ç—‡çŠ¶**: å®¹å™¨OOM (Out of Memory)

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# åœ¨docker-compose.ymlä¸­é™åˆ¶å†…å­˜
services:
  stt-service:
    mem_limit: 8g
    memswap_limit: 8g
```

### é—®é¢˜4: ç«¯å£å†²çª

**ç—‡çŠ¶**: ç«¯å£å·²è¢«å ç”¨

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
lsof -i :8000
lsof -i :8001
lsof -i :8002

# ä¿®æ”¹docker-compose.ymlä¸­çš„ç«¯å£æ˜ å°„
ports:
  - "18000:8000"  # ä½¿ç”¨å…¶ä»–ç«¯å£
```

## ğŸ” ç”Ÿäº§éƒ¨ç½²å»ºè®®

### 1. ä½¿ç”¨åå‘ä»£ç†
```nginx
# Nginxé…ç½®ç¤ºä¾‹
upstream opentalker {
    server localhost:8000;
}

server {
    listen 80;
    server_name api.example.com;

    location / {
        proxy_pass http://opentalker;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### 2. æ·»åŠ è®¤è¯
```python
# åœ¨Gatewayä¸­æ·»åŠ API KeyéªŒè¯
from fastapi import Security, HTTPException
from fastapi.security import APIKeyHeader

api_key_header = APIKeyHeader(name="X-API-Key")

async def verify_api_key(api_key: str = Security(api_key_header)):
    if api_key != os.getenv("API_KEY"):
        raise HTTPException(status_code=403, detail="Invalid API Key")
```

### 3. é…ç½®æ—¥å¿—
```yaml
# docker-compose.yml
services:
  gateway:
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
```

### 4. å¥åº·æ£€æŸ¥å’Œè‡ªåŠ¨é‡å¯
```yaml
# docker-compose.yml
services:
  stt-service:
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [Dockerå®˜æ–¹æ–‡æ¡£](https://docs.docker.com/)
- [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-docker)
- [OpenTalker GitHub](https://github.com/DDKTopLabs/OpenTalker)
- [Qwen3-ASR](https://github.com/QwenLM/Qwen3-ASR)
- [Qwen3-TTS](https://github.com/QwenLM/Qwen3-TTS)

---

**ç‰ˆæœ¬**: v0.3.0  
**æ›´æ–°æ—¶é—´**: 2026-02-04  
**ç»´æŠ¤è€…**: DDKTopLabs
