# OpenTalker

OpenAI å…¼å®¹çš„éŸ³é¢‘å¤„ç† API æœåŠ¡ï¼Œæ”¯æŒè¯­éŸ³è½¬æ–‡å­—ï¼ˆSTTï¼‰å’Œæ–‡å­—è½¬è¯­éŸ³ï¼ˆTTSï¼‰ï¼Œä¸“ä¸º GTX 1050 Tiï¼ˆ4GB æ˜¾å­˜ï¼‰ä¼˜åŒ–ã€‚

[![CI](https://github.com/DDKTopLabs/OpenTalker/actions/workflows/ci.yml/badge.svg)](https://github.com/DDKTopLabs/OpenTalker/actions/workflows/ci.yml)
[![Docker Build](https://github.com/DDKTopLabs/OpenTalker/actions/workflows/docker.yml/badge.svg)](https://github.com/DDKTopLabs/OpenTalker/actions/workflows/docker.yml)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![CUDA](https://img.shields.io/badge/CUDA-12.1%2F12.3-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![Docker Pulls](https://img.shields.io/docker/pulls/ddktoplabs/opentalker)](https://hub.docker.com/r/ddktoplabs/opentalker)

## âœ¨ ç‰¹æ€§

- ğŸ¯ **å®Œå…¨å…¼å®¹ OpenAI Audio API** - æ— ç¼æ›¿æ¢ OpenAI çš„ `/v1/audio/*` ç«¯ç‚¹
- ğŸš€ **æ™ºèƒ½æ¨¡å‹ç®¡ç†** - è‡ªåŠ¨åˆ‡æ¢ STT/TTS æ¨¡å‹ï¼Œç¡®ä¿ 4GB æ˜¾å­˜å†…è¿è¡Œ
- ğŸ¤ **é«˜è´¨é‡ STT** - ä½¿ç”¨ Qwen3-ASR-0.6Bï¼Œæ”¯æŒå¤šè¯­è¨€å’Œæ—¶é—´æˆ³ç”Ÿæˆ
- ğŸ—£ï¸ **å…ˆè¿› TTS** - ä½¿ç”¨ IndexTTS2ï¼Œæ”¯æŒè¯­éŸ³å…‹éš†å’Œæƒ…æ„Ÿæ§åˆ¶
- ğŸ³ **Docker éƒ¨ç½²** - ä¸€é”®éƒ¨ç½²ï¼ŒåŒ…å« GPU æ”¯æŒ
- ğŸ‡¨ğŸ‡³ **å›½å†…é•œåƒä¼˜åŒ–** - ä½¿ç”¨æ¸…åå¤§å­¦é•œåƒæºï¼Œä¸‹è½½é€Ÿåº¦å¿«
- ğŸ“Š **æ€§èƒ½ç›‘æ§** - GPU ç›‘æ§ã€æ€§èƒ½ç»Ÿè®¡ã€å¥åº·æ£€æŸ¥
- ğŸ”§ **çµæ´»é…ç½®** - ç¯å¢ƒå˜é‡é…ç½®ï¼Œæ”¯æŒå¤šç§éŸ³é¢‘æ ¼å¼

## ğŸ“‹ ç›®å½•

- [ç¡¬ä»¶è¦æ±‚](#ç¡¬ä»¶è¦æ±‚)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [å®‰è£…æŒ‡å—](#å®‰è£…æŒ‡å—)
  - [Docker éƒ¨ç½²ï¼ˆæ¨èï¼‰](#docker-éƒ¨ç½²æ¨è)
  - [æœ¬åœ°å®‰è£…](#æœ¬åœ°å®‰è£…)
- [API æ–‡æ¡£](#api-æ–‡æ¡£)
- [é…ç½®å‚è€ƒ](#é…ç½®å‚è€ƒ)
- [é•œåƒæºé…ç½®](#é•œåƒæºé…ç½®)
- [æ€§èƒ½é¢„æœŸ](#æ€§èƒ½é¢„æœŸ)
- [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
- [è®¸å¯è¯](#è®¸å¯è¯)

## ğŸ–¥ï¸ ç¡¬ä»¶è¦æ±‚

### æœ€ä½è¦æ±‚
- **GPU**: NVIDIA GTX 1050 Ti (4GB VRAM) æˆ–æ›´é«˜
- **CPU**: 4 æ ¸å¿ƒ
- **å†…å­˜**: 8GB RAM
- **å­˜å‚¨**: 20GB å¯ç”¨ç©ºé—´ï¼ˆç”¨äºæ¨¡å‹ï¼‰
- **CUDA**: 12.1 æˆ– 12.3

### æ¨èé…ç½®
- **GPU**: NVIDIA RTX 3060 (12GB VRAM) æˆ–æ›´é«˜
- **CPU**: 8 æ ¸å¿ƒ
- **å†…å­˜**: 16GB RAM
- **å­˜å‚¨**: 50GB SSD

### è½¯ä»¶è¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Ubuntu 22.04 / 20.04
- **Docker**: 20.10+ (Docker éƒ¨ç½²)
- **NVIDIA Driver**: 525+ (æ”¯æŒ CUDA 12.x)
- **NVIDIA Container Toolkit**: æœ€æ–°ç‰ˆæœ¬

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ä½¿ç”¨ Dockerï¼ˆæ¨èï¼‰

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/DDKTopLabs/OpenTalker.git
cd OpenTalker

# 2. ä¸‹è½½æ¨¡å‹ï¼ˆä½¿ç”¨ HF-Mirrorï¼Œå›½å†…é€Ÿåº¦å¿«ï¼‰
./scripts/download_models.sh

# 3. å¯åŠ¨æœåŠ¡
docker-compose up -d

# 4. æ£€æŸ¥çŠ¶æ€
curl http://localhost:8000/health

# 5. æµ‹è¯• STT
curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F "file=@audio.mp3" \
  -F "model=qwen3-asr-0.6b"

# 6. æµ‹è¯• TTS
curl -X POST http://localhost:8000/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{
    "model": "indextts-2",
    "input": "ä½ å¥½ï¼Œä¸–ç•Œï¼",
    "voice": "<base64_encoded_reference_audio>"
  }' \
  --output speech.wav
```

### ä½¿ç”¨ Python å®¢æˆ·ç«¯

```python
import openai

# é…ç½® API ç«¯ç‚¹
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "dummy"  # ä¸éœ€è¦çœŸå® API key

# STT - è¯­éŸ³è½¬æ–‡å­—
with open("audio.mp3", "rb") as audio_file:
    transcript = openai.Audio.transcribe(
        model="qwen3-asr-0.6b",
        file=audio_file,
        response_format="json"
    )
    print(transcript.text)

# TTS - æ–‡å­—è½¬è¯­éŸ³
import base64

# è¯»å–å‚è€ƒéŸ³é¢‘
with open("reference.wav", "rb") as f:
    voice_data = base64.b64encode(f.read()).decode()

response = openai.Audio.create_speech(
    model="indextts-2",
    input="ä½ å¥½ï¼Œä¸–ç•Œï¼",
    voice=voice_data
)

# ä¿å­˜éŸ³é¢‘
with open("output.wav", "wb") as f:
    f.write(response.content)
```

## ğŸ“¦ å®‰è£…æŒ‡å—

### Docker éƒ¨ç½²ï¼ˆæ¨èï¼‰

#### 1. å®‰è£… NVIDIA Container Toolkit

```bash
# Ubuntu/Debian
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

#### 2. éªŒè¯ GPU æ”¯æŒ

```bash
docker run --rm --gpus all nvidia/cuda:12.3.2-base-ubuntu22.04 nvidia-smi
```

#### 3. ä¸‹è½½æ¨¡å‹

```bash
# ä½¿ç”¨ Bash è„šæœ¬ï¼ˆæ¨èï¼‰
./scripts/download_models.sh

# æˆ–ä½¿ç”¨ Python è„šæœ¬
python scripts/init_models.py --include-aligner
```

#### 4. é…ç½®ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰

```bash
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶ä¿®æ”¹é…ç½®
```

#### 5. å¯åŠ¨æœåŠ¡

```bash
# æ„å»ºå¹¶å¯åŠ¨
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# åœæ­¢æœåŠ¡
docker-compose down
```

### æœ¬åœ°å®‰è£…

#### 1. å®‰è£…ç³»ç»Ÿä¾èµ–

```bash
# Ubuntu 22.04
sudo apt-get update
sudo apt-get install -y \
  python3.11 \
  python3.11-dev \
  ffmpeg \
  libsndfile1 \
  libsndfile1-dev \
  build-essential
```

#### 2. å®‰è£… uv åŒ…ç®¡ç†å™¨

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.cargo/env
```

#### 3. å®‰è£… Python ä¾èµ–

```bash
# ä½¿ç”¨ uvï¼ˆæ¨èï¼Œè‡ªåŠ¨ä½¿ç”¨æ¸…åé•œåƒï¼‰
uv pip install -e .

# æˆ–ä½¿ç”¨ pip
pip install -e .
```

#### 4. ä¸‹è½½æ¨¡å‹

```bash
./scripts/download_models.sh
```

#### 5. å¯åŠ¨æœåŠ¡

```bash
# å¼€å‘æ¨¡å¼
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

# ç”Ÿäº§æ¨¡å¼
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 1
```

## ğŸ“š API æ–‡æ¡£

### ç«¯ç‚¹æ¦‚è§ˆ

| ç«¯ç‚¹ | æ–¹æ³• | æè¿° |
|------|------|------|
| `/v1/audio/transcriptions` | POST | è¯­éŸ³è½¬æ–‡å­—ï¼ˆSTTï¼‰ |
| `/v1/audio/speech` | POST | æ–‡å­—è½¬è¯­éŸ³ï¼ˆTTSï¼‰ |
| `/v1/models` | GET | åˆ—å‡ºå¯ç”¨æ¨¡å‹ |
| `/health` | GET | å¥åº·æ£€æŸ¥ |
| `/metrics` | GET | æ€§èƒ½æŒ‡æ ‡ |

### STT - è¯­éŸ³è½¬æ–‡å­—

**ç«¯ç‚¹**: `POST /v1/audio/transcriptions`

**è¯·æ±‚å‚æ•°**:

| å‚æ•° | ç±»å‹ | å¿…éœ€ | æè¿° |
|------|------|------|------|
| `file` | file | æ˜¯ | éŸ³é¢‘æ–‡ä»¶ï¼ˆMP3, WAV, FLAC, M4A, OGG, WEBMï¼‰ |
| `model` | string | å¦ | æ¨¡å‹åç§°ï¼ˆé»˜è®¤: `qwen3-asr-0.6b`ï¼‰ |
| `language` | string | å¦ | è¯­è¨€ä»£ç ï¼ˆISO-639-1ï¼Œå¦‚ `zh`, `en`ï¼‰ |
| `response_format` | string | å¦ | å“åº”æ ¼å¼ï¼ˆ`json`, `text`, `srt`, `vtt`, `verbose_json`ï¼‰ |
| `temperature` | float | å¦ | é‡‡æ ·æ¸©åº¦ï¼ˆ0.0-1.0ï¼Œé»˜è®¤: 0.0ï¼‰ |
| `timestamp_granularities` | string | å¦ | æ—¶é—´æˆ³ç²’åº¦ï¼ˆ`word`, `segment`ï¼Œé€—å·åˆ†éš”ï¼‰ |

**å“åº”ç¤ºä¾‹**:

```json
{
  "text": "ä½ å¥½ï¼Œä¸–ç•Œï¼"
}
```

**Verbose JSON å“åº”**:

```json
{
  "task": "transcribe",
  "language": "zh",
  "duration": 2.5,
  "text": "ä½ å¥½ï¼Œä¸–ç•Œï¼",
  "segments": [
    {
      "id": 0,
      "start": 0.0,
      "end": 2.5,
      "text": "ä½ å¥½ï¼Œä¸–ç•Œï¼"
    }
  ],
  "words": [
    {"word": "ä½ å¥½", "start": 0.0, "end": 1.2},
    {"word": "ä¸–ç•Œ", "start": 1.5, "end": 2.5}
  ]
}
```

**cURL ç¤ºä¾‹**:

```bash
curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F "file=@audio.mp3" \
  -F "model=qwen3-asr-0.6b" \
  -F "response_format=json" \
  -F "language=zh"
```

### TTS - æ–‡å­—è½¬è¯­éŸ³

**ç«¯ç‚¹**: `POST /v1/audio/speech`

**è¯·æ±‚å‚æ•°**:

| å‚æ•° | ç±»å‹ | å¿…éœ€ | æè¿° |
|------|------|------|------|
| `model` | string | å¦ | æ¨¡å‹åç§°ï¼ˆé»˜è®¤: `indextts-2`ï¼‰ |
| `input` | string | æ˜¯ | è¦åˆæˆçš„æ–‡æœ¬ï¼ˆ1-4096 å­—ç¬¦ï¼‰ |
| `voice` | string | æ˜¯ | Base64 ç¼–ç çš„å‚è€ƒéŸ³é¢‘ï¼ˆç”¨äºè¯­éŸ³å…‹éš†ï¼‰ |
| `response_format` | string | å¦ | éŸ³é¢‘æ ¼å¼ï¼ˆ`wav`, `mp3`, `flac`, `opus`ï¼‰ |
| `speed` | float | å¦ | è¯­é€Ÿï¼ˆ0.25-4.0ï¼Œé»˜è®¤: 1.0ï¼‰ |
| `emotion` | object | å¦ | æƒ…æ„Ÿæ§åˆ¶é…ç½® |

**æƒ…æ„Ÿæ§åˆ¶å‚æ•°**:

```json
{
  "mode": "auto",  // auto, audio, vector, text
  "alpha": 1.0,    // æƒ…æ„Ÿå¼ºåº¦ (0.0-1.0)
  "audio": "<base64_audio>",  // æƒ…æ„Ÿå‚è€ƒéŸ³é¢‘
  "vector": [0.1, 0.2, ...],  // æƒ…æ„Ÿå‘é‡
  "text": "å¼€å¿ƒ"               // æƒ…æ„Ÿæ–‡æœ¬æè¿°
}
```

**è¯·æ±‚ç¤ºä¾‹**:

```json
{
  "model": "indextts-2",
  "input": "ä½ å¥½ï¼Œä¸–ç•Œï¼è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚",
  "voice": "UklGRiQAAABXQVZFZm10...",
  "response_format": "wav",
  "speed": 1.0,
  "emotion": {
    "mode": "auto",
    "alpha": 1.0
  }
}
```

**cURL ç¤ºä¾‹**:

```bash
# å‡†å¤‡å‚è€ƒéŸ³é¢‘
VOICE_BASE64=$(base64 -w 0 reference.wav)

# å‘é€è¯·æ±‚
curl -X POST http://localhost:8000/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d "{
    \"model\": \"indextts-2\",
    \"input\": \"ä½ å¥½ï¼Œä¸–ç•Œï¼\",
    \"voice\": \"$VOICE_BASE64\",
    \"response_format\": \"wav\"
  }" \
  --output output.wav
```

### å¥åº·æ£€æŸ¥

**ç«¯ç‚¹**: `GET /health`

**å“åº”ç¤ºä¾‹**:

```json
{
  "status": "healthy",
  "gpu": {
    "device_name": "NVIDIA GeForce GTX 1050 Ti",
    "total_memory_mb": 4096.0,
    "used_memory_mb": 1234.5,
    "free_memory_mb": 2861.5,
    "utilization_percent": 30.1
  },
  "model": {
    "model_type": "stt",
    "status": "loaded",
    "model_name": "Qwen/Qwen3-ASR-0.6B"
  }
}
```

### åˆ—å‡ºæ¨¡å‹

**ç«¯ç‚¹**: `GET /v1/models`

**å“åº”ç¤ºä¾‹**:

```json
{
  "object": "list",
  "data": [
    {
      "id": "qwen3-asr-0.6b",
      "object": "model",
      "created": 1704067200,
      "owned_by": "qwen"
    },
    {
      "id": "indextts-2",
      "object": "model",
      "created": 1704067200,
      "owned_by": "indextts"
    }
  ]
}
```

## âš™ï¸ é…ç½®å‚è€ƒ

### ç¯å¢ƒå˜é‡

æ‰€æœ‰é…ç½®é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼Œå¯ä»¥åœ¨ `.env` æ–‡ä»¶æˆ– `docker-compose.yml` ä¸­é…ç½®ã€‚

#### HuggingFace é•œåƒ

```bash
# HuggingFace é•œåƒç«¯ç‚¹ï¼ˆå›½å†…ä½¿ç”¨ HF-Mirrorï¼‰
HF_ENDPOINT=https://hf-mirror.com

# æ¨¡å‹ç¼“å­˜ç›®å½•
HUGGINGFACE_HUB_CACHE=/models/.cache/huggingface
```

#### STT é…ç½®ï¼ˆQwen3-ASRï¼‰

```bash
# æ¨¡å‹æ ‡è¯†
QWEN_ASR_MODEL=Qwen/Qwen3-ASR-0.6B

# åç«¯ï¼ˆtransformers æˆ– onnxï¼‰
QWEN_ASR_BACKEND=transformers

# æ•°æ®ç±»å‹ï¼ˆfloat16 æˆ– float32ï¼‰
QWEN_ASR_DTYPE=float16

# è®¾å¤‡ï¼ˆcuda:0 æˆ– cpuï¼‰
QWEN_ASR_DEVICE=cuda:0

# å¯ç”¨å¼ºåˆ¶å¯¹é½å™¨ï¼ˆauto, true, falseï¼‰
QWEN_ASR_ENABLE_ALIGNER=auto

# æœ€å¤§æ‰¹å¤„ç†å¤§å°
QWEN_ASR_MAX_BATCH_SIZE=8
```

#### TTS é…ç½®ï¼ˆIndexTTS2ï¼‰

```bash
# æ¨¡å‹ç›®å½•
INDEXTTS_MODEL_DIR=/models/indextts

# ä½¿ç”¨ FP16 ç²¾åº¦
INDEXTTS_USE_FP16=true

# ä½¿ç”¨ CUDA å†…æ ¸ä¼˜åŒ–ï¼ˆéœ€è¦ç¼–è¯‘ï¼‰
INDEXTTS_USE_CUDA_KERNEL=false

# ä½¿ç”¨ DeepSpeed
INDEXTTS_USE_DEEPSPEED=false
```

#### æœåŠ¡é…ç½®

```bash
# API ä¸»æœº
API_HOST=0.0.0.0

# API ç«¯å£
API_PORT=8000

# æ—¥å¿—çº§åˆ«ï¼ˆDEBUG, INFO, WARNING, ERROR, CRITICALï¼‰
LOG_LEVEL=INFO

# æœ€å¤§ä¸Šä¼ æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
MAX_UPLOAD_SIZE=52428800  # 50MB
```

#### GPU é…ç½®

```bash
# CUDA å¯è§è®¾å¤‡
CUDA_VISIBLE_DEVICES=0
```

#### æ¨¡å‹ç®¡ç†

```bash
# æ¨¡å‹åˆ‡æ¢è¶…æ—¶ï¼ˆç§’ï¼‰
MODEL_SWITCH_TIMEOUT=30

# å¯ç”¨æ¨¡å‹é¢„åŠ è½½
ENABLE_MODEL_PRELOAD=false

# é»˜è®¤é¢„åŠ è½½æ¨¡å‹ï¼ˆnone, stt, ttsï¼‰
DEFAULT_PRELOAD_MODEL=none
```

## ğŸ”§ é•œåƒæºé…ç½®

æœ¬é¡¹ç›®é’ˆå¯¹å›½å†…ç½‘ç»œç¯å¢ƒä¼˜åŒ–ï¼Œä½¿ç”¨æ¸…åå¤§å­¦é•œåƒæºã€‚

### PyPI é•œåƒï¼ˆPython åŒ…ï¼‰

é…ç½®åœ¨ `pyproject.toml` ä¸­ï¼š

```toml
[[tool.uv.index]]
name = "tsinghua-pypi"
url = "https://pypi.tuna.tsinghua.edu.cn/simple"
default = true
```

### PyTorch é•œåƒï¼ˆCUDA 12.1ï¼‰

```toml
[tool.uv.sources]
torch = { index = "tsinghua-pytorch" }
torchaudio = { index = "tsinghua-pytorch" }

[[tool.uv.index]]
name = "tsinghua-pytorch"
url = "https://mirrors.tuna.tsinghua.edu.cn/pytorch/whl/cu121"
explicit = true
```

### HuggingFace é•œåƒï¼ˆæ¨¡å‹ä¸‹è½½ï¼‰

```bash
export HF_ENDPOINT=https://hf-mirror.com
```

### Ubuntu APT é•œåƒ

Dockerfile ä¸­è‡ªåŠ¨é…ç½®ï¼š

```dockerfile
RUN sed -i 's|http://archive.ubuntu.com|https://mirrors.tuna.tsinghua.edu.cn|g' /etc/apt/sources.list
```

### CUDA ç‰ˆæœ¬åˆ‡æ¢

å¦‚éœ€åˆ‡æ¢ CUDA ç‰ˆæœ¬ï¼Œä¿®æ”¹ `pyproject.toml`ï¼š

```toml
# CUDA 12.1 (é»˜è®¤)
[[tool.uv.index]]
name = "tsinghua-pytorch"
url = "https://mirrors.tuna.tsinghua.edu.cn/pytorch/whl/cu121"

# CUDA 11.8
[[tool.uv.index]]
name = "tsinghua-pytorch"
url = "https://mirrors.tuna.tsinghua.edu.cn/pytorch/whl/cu118"

# CPU only
[[tool.uv.index]]
name = "tsinghua-pytorch"
url = "https://mirrors.tuna.tsinghua.edu.cn/pytorch/whl/cpu"
```

åŒæ—¶æ›´æ–° Dockerfile åŸºç¡€é•œåƒï¼š

```dockerfile
# CUDA 12.1
FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

# CUDA 11.8
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04
```

## ğŸ“Š æ€§èƒ½é¢„æœŸ

### GTX 1050 Ti (4GB VRAM)

| æ“ä½œ | å»¶è¿Ÿ | ååé‡ | æ˜¾å­˜å ç”¨ |
|------|------|--------|----------|
| STT (Qwen3-ASR-0.6B) | 0.5-2s | ~30s éŸ³é¢‘/s | ~2.5GB |
| TTS (IndexTTS2) | 1-3s | ~10 å­—ç¬¦/s | ~3.0GB |
| æ¨¡å‹åˆ‡æ¢ | 5-10s | - | - |

### RTX 3060 (12GB VRAM)

| æ“ä½œ | å»¶è¿Ÿ | ååé‡ | æ˜¾å­˜å ç”¨ |
|------|------|--------|----------|
| STT (Qwen3-ASR-0.6B) | 0.2-1s | ~60s éŸ³é¢‘/s | ~2.5GB |
| TTS (IndexTTS2) | 0.5-1.5s | ~20 å­—ç¬¦/s | ~3.0GB |
| æ¨¡å‹åˆ‡æ¢ | 3-5s | - | - |

### æ³¨æ„äº‹é¡¹

- **æ¨¡å‹åˆ‡æ¢**: ç”±äº 4GB æ˜¾å­˜é™åˆ¶ï¼ŒSTT å’Œ TTS ä¸èƒ½åŒæ—¶åŠ è½½ï¼Œåˆ‡æ¢éœ€è¦ 5-10 ç§’
- **æ‰¹å¤„ç†**: ä¸æ”¯æŒæ‰¹å¤„ç†ï¼Œä¸€æ¬¡åªèƒ½å¤„ç†ä¸€ä¸ªè¯·æ±‚
- **å¹¶å‘**: è¯·æ±‚ä¼šæ’é˜Ÿå¤„ç†ï¼Œä¸æ”¯æŒçœŸæ­£çš„å¹¶å‘
- **é•¿éŸ³é¢‘**: è¶…è¿‡ 30 ç§’çš„éŸ³é¢‘å¯èƒ½éœ€è¦æ›´é•¿å¤„ç†æ—¶é—´
- **é•¿æ–‡æœ¬**: TTS ä¼šè‡ªåŠ¨åˆ†æ®µå¤„ç†è¶…è¿‡ 1000 å­—ç¬¦çš„æ–‡æœ¬

## ğŸ” æ•…éšœæ’æŸ¥

### 1. GPU ä¸å¯ç”¨

**ç—‡çŠ¶**: `CUDA not available` æˆ– `No CUDA device`

**è§£å†³æ–¹æ¡ˆ**:

```bash
# æ£€æŸ¥ NVIDIA é©±åŠ¨
nvidia-smi

# æ£€æŸ¥ Docker GPU æ”¯æŒ
docker run --rm --gpus all nvidia/cuda:12.3.2-base-ubuntu22.04 nvidia-smi

# é‡æ–°å®‰è£… NVIDIA Container Toolkit
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### 2. æ¨¡å‹ä¸‹è½½å¤±è´¥

**ç—‡çŠ¶**: `Connection timeout` æˆ– `Failed to download`

**è§£å†³æ–¹æ¡ˆ**:

```bash
# ä½¿ç”¨ HF-Mirrorï¼ˆå›½å†…ï¼‰
export HF_ENDPOINT=https://hf-mirror.com

# é‡æ–°ä¸‹è½½
./scripts/download_models.sh

# æˆ–æ‰‹åŠ¨ä¸‹è½½
huggingface-cli download Qwen/Qwen3-ASR-0.6B --local-dir ./models/qwen3-asr
```

### 3. æ˜¾å­˜ä¸è¶³

**ç—‡çŠ¶**: `CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:

```bash
# æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨
nvidia-smi

# ç¡®ä¿åªæœ‰ä¸€ä¸ªæ¨¡å‹åŠ è½½
# æ£€æŸ¥ /health ç«¯ç‚¹æŸ¥çœ‹å½“å‰æ¨¡å‹çŠ¶æ€

# é™ä½æ‰¹å¤„ç†å¤§å°
export QWEN_ASR_MAX_BATCH_SIZE=4

# ä½¿ç”¨ float32 ä»£æ›¿ float16ï¼ˆéœ€è¦æ›´å¤šæ˜¾å­˜ï¼‰
export QWEN_ASR_DTYPE=float32
```

### 4. æ¨¡å‹åŠ è½½è¶…æ—¶

**ç—‡çŠ¶**: `Model loading timeout`

**è§£å†³æ–¹æ¡ˆ**:

```bash
# å¢åŠ è¶…æ—¶æ—¶é—´
export MODEL_SWITCH_TIMEOUT=60

# é¢„åŠ è½½æ¨¡å‹ï¼ˆå¯åŠ¨æ—¶åŠ è½½ï¼‰
export ENABLE_MODEL_PRELOAD=true
export DEFAULT_PRELOAD_MODEL=stt  # æˆ– tts
```

### 5. éŸ³é¢‘æ ¼å¼ä¸æ”¯æŒ

**ç—‡çŠ¶**: `Invalid audio file` æˆ– `Unsupported format`

**è§£å†³æ–¹æ¡ˆ**:

```bash
# å®‰è£… ffmpeg
sudo apt-get install -y ffmpeg

# è½¬æ¢éŸ³é¢‘æ ¼å¼
ffmpeg -i input.m4a -ar 16000 -ac 1 output.wav

# æ”¯æŒçš„æ ¼å¼: MP3, WAV, FLAC, M4A, OGG, WEBM
```

### 6. Docker å®¹å™¨æ— æ³•å¯åŠ¨

**ç—‡çŠ¶**: å®¹å™¨å¯åŠ¨åç«‹å³é€€å‡º

**è§£å†³æ–¹æ¡ˆ**:

```bash
# æŸ¥çœ‹æ—¥å¿—
docker-compose logs

# æ£€æŸ¥é…ç½®
docker-compose config

# é‡æ–°æ„å»º
docker-compose build --no-cache
docker-compose up -d
```

### 7. API å“åº”æ…¢

**ç—‡çŠ¶**: è¯·æ±‚è¶…æ—¶æˆ–å“åº”æ—¶é—´é•¿

**è§£å†³æ–¹æ¡ˆ**:

- é¦–æ¬¡è¯·æ±‚ä¼šè§¦å‘æ¨¡å‹åŠ è½½ï¼ˆ5-10 ç§’ï¼‰
- åç»­è¯·æ±‚åº”è¯¥æ›´å¿«
- æ£€æŸ¥ GPU åˆ©ç”¨ç‡: `nvidia-smi`
- æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡: `curl http://localhost:8000/metrics`

### 8. ä¾èµ–å®‰è£…å¤±è´¥

**ç—‡çŠ¶**: `pip install` å¤±è´¥æˆ–è¶…æ—¶

**è§£å†³æ–¹æ¡ˆ**:

```bash
# ä½¿ç”¨ uvï¼ˆæ›´å¿«ï¼Œè‡ªåŠ¨ä½¿ç”¨é•œåƒï¼‰
curl -LsSf https://astral.sh/uv/install.sh | sh
uv pip install -e .

# æˆ–æ‰‹åŠ¨é…ç½® pip é•œåƒ
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
pip install -e .
```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### Python å®Œæ•´ç¤ºä¾‹

```python
import openai
import base64
from pathlib import Path

# é…ç½®
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "dummy"

def transcribe_audio(audio_path: str) -> str:
    """è¯­éŸ³è½¬æ–‡å­—"""
    with open(audio_path, "rb") as audio_file:
        transcript = openai.Audio.transcribe(
            model="qwen3-asr-0.6b",
            file=audio_file,
            response_format="verbose_json",
            timestamp_granularities=["word", "segment"]
        )
    return transcript

def synthesize_speech(text: str, reference_audio: str, output_path: str):
    """æ–‡å­—è½¬è¯­éŸ³"""
    # è¯»å–å‚è€ƒéŸ³é¢‘
    with open(reference_audio, "rb") as f:
        voice_data = base64.b64encode(f.read()).decode()
    
    # åˆæˆè¯­éŸ³
    response = openai.Audio.create_speech(
        model="indextts-2",
        input=text,
        voice=voice_data,
        response_format="wav",
        speed=1.0
    )
    
    # ä¿å­˜
    with open(output_path, "wb") as f:
        f.write(response.content)

# ä½¿ç”¨
if __name__ == "__main__":
    # STT
    result = transcribe_audio("input.mp3")
    print(f"Transcription: {result.text}")
    print(f"Duration: {result.duration}s")
    
    # TTS
    synthesize_speech(
        text="ä½ å¥½ï¼Œä¸–ç•Œï¼è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚",
        reference_audio="reference.wav",
        output_path="output.wav"
    )
    print("Speech synthesized!")
```

### cURL å®Œæ•´ç¤ºä¾‹

```bash
#!/bin/bash

API_BASE="http://localhost:8000"

# 1. å¥åº·æ£€æŸ¥
echo "Checking health..."
curl -s "$API_BASE/health" | jq .

# 2. åˆ—å‡ºæ¨¡å‹
echo -e "\nListing models..."
curl -s "$API_BASE/v1/models" | jq .

# 3. STT - è¯­éŸ³è½¬æ–‡å­—
echo -e "\nTranscribing audio..."
curl -X POST "$API_BASE/v1/audio/transcriptions" \
  -F "file=@audio.mp3" \
  -F "model=qwen3-asr-0.6b" \
  -F "response_format=verbose_json" \
  -F "timestamp_granularities=word,segment" \
  | jq .

# 4. TTS - æ–‡å­—è½¬è¯­éŸ³
echo -e "\nSynthesizing speech..."
VOICE_BASE64=$(base64 -w 0 reference.wav)

curl -X POST "$API_BASE/v1/audio/speech" \
  -H "Content-Type: application/json" \
  -d "{
    \"model\": \"indextts-2\",
    \"input\": \"ä½ å¥½ï¼Œä¸–ç•Œï¼\",
    \"voice\": \"$VOICE_BASE64\",
    \"response_format\": \"wav\",
    \"speed\": 1.0
  }" \
  --output output.wav

echo "Done!"
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ Apache License 2.0 è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- [Qwen3-ASR](https://github.com/QwenLM/Qwen-Audio) - é«˜è´¨é‡è¯­éŸ³è¯†åˆ«æ¨¡å‹
- [IndexTTS2](https://github.com/IndexTeam/IndexTTS) - å…ˆè¿›çš„è¯­éŸ³åˆæˆæ¨¡å‹
- [FastAPI](https://fastapi.tiangolo.com/) - ç°ä»£ Web æ¡†æ¶
- [uv](https://github.com/astral-sh/uv) - å¿«é€Ÿ Python åŒ…ç®¡ç†å™¨
- æ¸…åå¤§å­¦å¼€æºè½¯ä»¶é•œåƒç«™ - æä¾›é•œåƒæœåŠ¡

## ğŸ“§ è”ç³»æ–¹å¼

- é¡¹ç›®ä¸»é¡µ: https://github.com/DDKTopLabs/OpenTalker
- é—®é¢˜åé¦ˆ: https://github.com/DDKTopLabs/OpenTalker/issues

## ğŸ—ºï¸ è·¯çº¿å›¾

- [ ] æ”¯æŒæµå¼ TTS è¾“å‡º
- [ ] æ”¯æŒæ‰¹å¤„ç† STT
- [ ] æ·»åŠ æ›´å¤š TTS æ¨¡å‹
- [ ] æ”¯æŒæ›´å¤šè¯­è¨€
- [ ] Web UI ç•Œé¢
- [ ] æ€§èƒ½ä¼˜åŒ–
- [ ] å®Œæ•´çš„æµ‹è¯•å¥—ä»¶

---

**æ³¨æ„**: æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚è¯·éµå®ˆç›¸å…³æ¨¡å‹çš„ä½¿ç”¨æ¡æ¬¾å’Œè®¸å¯è¯ã€‚
